{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86284ac9",
   "metadata": {},
   "source": [
    "# Auto Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69d04211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a15d3db",
   "metadata": {},
   "source": [
    "This project is going to use the COCO annotations dataset. There are multiple captions available for each image, but to keep the training data manageable I will only use 2 captions per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c4ae53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.77s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "Length of training set: 236574\n",
      "Length of validation set: 10000\n",
      "torch.Size([3, 224, 224])\n",
      "A meal is presented in brightly colored plastic trays.\n"
     ]
    }
   ],
   "source": [
    "data_path=r\"C:\\Users\\water\\Documents\\datasets\\coco\"\n",
    "json_path=r\"C:\\Users\\water\\Documents\\datasets\\coco\\annotations_trainval2017\\annotations\"\n",
    "\n",
    "CAPS_PER_IMAGE = 2\n",
    "\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, root_dir, ann_file, caps_per_image=1, transform=None, target_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.ann_file = ann_file\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        self.coco = COCO(self.ann_file)\n",
    "        coco = self.coco\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.imgs = []\n",
    "        self.caps = []\n",
    "        \n",
    "        for i, _id in enumerate(self.ids):\n",
    "            ann_ids = coco.getAnnIds(imgIds=_id)\n",
    "            anns = coco.loadAnns(ann_ids)\n",
    "            \n",
    "            for i, ann in enumerate(anns):\n",
    "                self.caps.append(ann['caption'])\n",
    "                self.imgs.append(coco.loadImgs(_id)[0]['file_name'])\n",
    "                if i + 1 == caps_per_image:\n",
    "                    break\n",
    "        \n",
    "        self.df = pd.DataFrame({'img_path': self.imgs, 'caption': self.caps})\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx:int):\n",
    "        path = self.df['img_path'].iloc[idx]\n",
    "        img = Image.open(os.path.join(self.root_dir, path)).convert('RGB')\n",
    "        target = self.df['caption'].iloc[idx]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "train_ds = CocoDataset(\n",
    "    root_dir=data_path + '\\\\train2017', \n",
    "    ann_file=json_path + '\\\\captions_train2017.json',\n",
    "    caps_per_image=CAPS_PER_IMAGE,\n",
    "    transform=transform\n",
    "    )\n",
    "\n",
    "val_ds = CocoDataset(\n",
    "    root_dir=data_path + '\\\\val2017', \n",
    "    ann_file=json_path + '\\\\captions_val2017.json',\n",
    "    caps_per_image=CAPS_PER_IMAGE,\n",
    "    transform=transform\n",
    "    )\n",
    "\n",
    "print('Length of training set:', len(train_ds))\n",
    "print('Length of validation set:', len(val_ds))\n",
    "\n",
    "img, caption = train_ds[1]\n",
    "print(img.size())\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29806389",
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = os.getcwd()\n",
    "def save_model(model):\n",
    "    model_path = os.path.join(CWD, 'caption_model.pt')\n",
    "    torch.save(model, model_path)\n",
    "\n",
    "def save_vocab(vocab):\n",
    "    vocab_path = os.path.join(CWD + 'vocab.pt')\n",
    "    torch.save(vocab, vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9feee77",
   "metadata": {},
   "source": [
    "To convert the captions into tensors, a vocabulary object will be built. The tokenizer used is the 'basic english' tokenizer from torchtext. Padding will be used to make all captions the same length and special tokens will be used to indicate the beginning and ending of a caption, as well as an unknown token symbol (a word that's not in the vocabulary.) The minimum frequency is somewhat arbitrary, I will use 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6f6d5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 3884\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "def tokenize(data_slice):\n",
    "    _, caption = data_slice\n",
    "    return tokenizer(caption)\n",
    "\n",
    "MIN_FREQ = 15\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "def build_vocab(vocab=None):\n",
    "    if not vocab:\n",
    "        vocab = build_vocab_from_iterator(\n",
    "            iterator=map(tokenize, train_ds),\n",
    "            min_freq=MIN_FREQ,\n",
    "            specials=special_symbols,\n",
    "        )\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab()\n",
    "save_vocab(vocab)\n",
    "print('Number of words in vocabulary:', len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c491633",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "itos = vocab.get_itos()\n",
    "\n",
    "def collate_fn(batch):\n",
    "    image_batch, target_batch = [], []\n",
    "    for image, caption in batch:\n",
    "        text_token_ids = text_pipeline(caption)\n",
    "        text_tensor = torch.cat((torch.tensor([BOS_IDX]),\n",
    "                                 torch.tensor(text_token_ids),\n",
    "                                 torch.tensor([EOS_IDX])))\n",
    "        target_batch.append(text_tensor)\n",
    "        image_batch.append(image.unsqueeze(0))\n",
    "        #print([itos[token] for token in text_tensor])\n",
    "        \n",
    "    image_batch = torch.cat(image_batch)\n",
    "    target_batch = pad_sequence(target_batch, batch_first=True, padding_value=PAD_IDX)\n",
    "    #for target in target_batch:\n",
    "        #print([itos[token] for token in target])\n",
    "    \n",
    "    return image_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dca0cd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor images, captions in train_loader:\\n    for caption in captions:\\n        for token in caption:\\n            print(itos[token])\\n    print(image.size())\\n    #print(caption)\\n    break\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "'''\n",
    "for images, captions in train_loader:\n",
    "    for caption in captions:\n",
    "        for token in caption:\n",
    "            print(itos[token])\n",
    "    print(image.size())\n",
    "    #print(caption)\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da3b4fd",
   "metadata": {},
   "source": [
    "Rather than building the model all in one class, I'm going to separate the encoder and decoder part of the model into different classes. The encoder will be a pretrained ResNet, the decoder will be an LSTM based network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa702e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_size:int=1024):\n",
    "        super(Encoder, self).__init__()\n",
    "    \n",
    "        # Pretrained image classifier ResNet-152\n",
    "        self.CNN = models.resnet152(pretrained=True, progress=False)\n",
    "        for param in self.CNN.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        self.CNN.classifier = nn.Linear(self.CNN.fc.in_features, self.CNN.fc.in_features)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        self.fc2 = nn.Linear(self.CNN.fc.out_features, embed_size)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        x = F.relu(self.CNN(images))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65c33820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.embed_size, hidden_size=self.hidden_size,\n",
    "                           num_layers=self.num_layers, batch_first=True)\n",
    "        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)\n",
    "        self.linear = nn.Linear(hidden_size, self.vocab_size)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "            embeddings = self.embed(captions)\n",
    "            #print('caption size:', captions.size())\n",
    "            #embeddings = torch.permute(embeddings, (1, 0, 2))\n",
    "            features = features.unsqueeze(1)\n",
    "            #print('features size:', features.size())\n",
    "            #print('embedding size:', embeddings.size())\n",
    "            embeddings = torch.cat((features, embeddings), dim=1)\n",
    "            hidden_states, cell_states = self.lstm(embeddings)\n",
    "            outputs = self.linear(hidden_states)\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36a66177",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(CaptionModel, self).__init__()\n",
    "        self.encoder = Encoder(embed_size)\n",
    "        self.decoder = Decoder(embed_size, hidden_size, vocab_size, num_layers)\n",
    "        \n",
    "    def forward(self, image, caption):\n",
    "        x = self.encoder(image)\n",
    "        x = self.decoder(x, caption)\n",
    "        return x\n",
    "    \n",
    "    def generate_caption(self, image, vocabulary, max_length=50):\n",
    "        generated_caption = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = self.encoder(image).unsqueeze(0)\n",
    "            cell_states = None\n",
    "            \n",
    "            for i in range (max_length):\n",
    "                hidden_states, cell_states = self.decoder.lstm(x, cell_states)\n",
    "                output = self.decoder.linear(hidden_states.squeeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "                print(predicted.shape)\n",
    "                print(predicted)\n",
    "                generated_caption.append(predicted[i])\n",
    "                x = self.decoder.embed(output.long()).unsqueeze(0)\n",
    "                \n",
    "                if itos[predicted[i]] == '<eos>':\n",
    "                    break\n",
    "        return [itos[i] for i in generated_caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dc2fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 512\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "\n",
    "model = CaptionModel(\n",
    "    embed_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    vocab_size=len(vocab),\n",
    "    num_layers=5)\n",
    "\n",
    "#model.load_state_dict(torch.load(os.path.join(CWD, 'caption_model.pt'))) \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28f93999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "    \n",
    "    for i, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        #print('inputs shape:', inputs.size())\n",
    "        #print('labels shape:', labels.size())\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels[:,:-1])\n",
    "        #print('outputs size:', outputs.size())\n",
    "        loss = criterion(outputs.reshape(-1, outputs.shape[2]), labels.reshape(-1))\n",
    "        #loss = criterion(score.reshape(-1, score.shape[2]), labels.reshape(-1))\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "            \n",
    "    return last_loss\n",
    "\n",
    "def val_one_epoch(train_loss):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs, labels[:,:-1])\n",
    "            loss = criterion(outputs.reshape(-1, outputs.shape[2]), labels.reshape(-1))\n",
    "            \n",
    "            running_loss += loss.item() \n",
    "            \n",
    "    val_loss = running_loss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(train_loss, val_loss))\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa4880dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\water\\AppData\\Local\\Temp\\ipykernel_19600\\145555903.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393cd539401249c1a6cb1042acb55995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1000 loss: 2.917600263595581\n",
      "  batch 2000 loss: 2.1631980882883073\n",
      "  batch 3000 loss: 1.9276047449707985\n",
      "  batch 4000 loss: 1.8256837568283082\n",
      "  batch 5000 loss: 1.7724254564642907\n",
      "  batch 6000 loss: 1.718541918039322\n",
      "  batch 7000 loss: 1.6709784666895866\n",
      "LOSS train 1.6709784666895866 valid 1.658398258038603\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "min_loss = 1000\n",
    "for epoch in range(EPOCHS):\n",
    "    print('Epoch number:', epoch)\n",
    "    train_loss = train_one_epoch()\n",
    "    val_loss = val_one_epoch(train_loss)\n",
    "    \n",
    "    if val_loss < min_loss:\n",
    "        save_model(model)\n",
    "print('Training Complete')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf1b612",
   "metadata": {},
   "source": [
    "Training and validation loss confirm no overfitting.\n",
    "\n",
    "TODO: Train for longer on cloud platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28af953b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([32])\n",
      "tensor([ 2,  4, 24,  4,  4,  7,  7,  1,  4,  7,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Git\\auto-caption\\auto_caption_v2.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Git/auto-caption/auto_caption_v2.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#print(outputs)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Git/auto-caption/auto_caption_v2.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(inputs[\u001b[39m1\u001b[39m, :, :, :]\u001b[39m.\u001b[39msize())\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Git/auto-caption/auto_caption_v2.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model\u001b[39m.\u001b[39;49mgenerate_caption(inputs, vocab)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Git/auto-caption/auto_caption_v2.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m num_test:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Git/auto-caption/auto_caption_v2.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;32mc:\\Git\\auto-caption\\auto_caption_v2.ipynb Cell 17\u001b[0m in \u001b[0;36mCaptionModel.generate_caption\u001b[1;34m(self, image, vocabulary, max_length)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Git/auto-caption/auto_caption_v2.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(predicted\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Git/auto-caption/auto_caption_v2.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mprint\u001b[39m(predicted)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Git/auto-caption/auto_caption_v2.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m generated_caption\u001b[39m.\u001b[39mappend(predicted\u001b[39m.\u001b[39;49mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Git/auto-caption/auto_caption_v2.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39membed(output\u001b[39m.\u001b[39mlong())\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Git/auto-caption/auto_caption_v2.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mif\u001b[39;00m itos[predicted\u001b[39m.\u001b[39mitem()] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m<eos>\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "num_test = 1\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(val_loader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(inputs, labels[:,:-1])\n",
    "        #print(outputs)\n",
    "        print(inputs[1, :, :, :].size())\n",
    "        model.generate_caption(inputs, vocab)\n",
    "        if i == num_test:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d468fb1a4afb28ced3325811818b75ea47b4773e231d19fb9e9ee0fa4eb4bbba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
