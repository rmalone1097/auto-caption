{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69d04211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52c4ae53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.75s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "Length of training set: 236574\n",
      "Length of validation set: 10000\n",
      "torch.Size([3, 224, 224])\n",
      "A meal is presented in brightly colored plastic trays.\n"
     ]
    }
   ],
   "source": [
    "data_path=r\"C:\\Users\\water\\Documents\\datasets\\coco\"\n",
    "json_path=r\"C:\\Users\\water\\Documents\\datasets\\coco\\annotations_trainval2017\\annotations\"\n",
    "\n",
    "CAPS_PER_IMAGE = 2\n",
    "\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, root_dir, ann_file, caps_per_image=1, transform=None, target_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.ann_file = ann_file\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        self.coco = COCO(self.ann_file)\n",
    "        coco = self.coco\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.imgs = []\n",
    "        self.caps = []\n",
    "        \n",
    "        for i, _id in enumerate(self.ids):\n",
    "            ann_ids = coco.getAnnIds(imgIds=_id)\n",
    "            anns = coco.loadAnns(ann_ids)\n",
    "            \n",
    "            for i, ann in enumerate(anns):\n",
    "                self.caps.append(ann['caption'])\n",
    "                self.imgs.append(coco.loadImgs(_id)[0]['file_name'])\n",
    "                if i + 1 == caps_per_image:\n",
    "                    break\n",
    "        \n",
    "        self.df = pd.DataFrame({'img_path': self.imgs, 'caption': self.caps})\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx:int):\n",
    "        path = self.df['img_path'].iloc[idx]\n",
    "        img = Image.open(os.path.join(self.root_dir, path)).convert('RGB')\n",
    "        target = self.df['caption'].iloc[idx]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "train_ds = CocoDataset(\n",
    "    root_dir=data_path + '\\\\train2017', \n",
    "    ann_file=json_path + '\\\\captions_train2017.json',\n",
    "    caps_per_image=CAPS_PER_IMAGE,\n",
    "    transform=transform\n",
    "    )\n",
    "\n",
    "val_ds = CocoDataset(\n",
    "    root_dir=data_path + '\\\\val2017', \n",
    "    ann_file=json_path + '\\\\captions_val2017.json',\n",
    "    caps_per_image=CAPS_PER_IMAGE,\n",
    "    transform=transform\n",
    "    )\n",
    "\n",
    "print('Length of training set:', len(train_ds))\n",
    "print('Length of validation set:', len(val_ds))\n",
    "\n",
    "img, caption = train_ds[1]\n",
    "print(img.size())\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29806389",
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = os.getcwd()\n",
    "def save_model(model):\n",
    "    model_path = os.path.join(CWD, 'caption_model.pt')\n",
    "    torch.save(model, model_path)\n",
    "\n",
    "def save_vocab(vocab):\n",
    "    vocab_path = os.path.join(CWD + 'vocab.pt')\n",
    "    torch.save(vocab, vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6f6d5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 3884\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "def tokenize(data_slice):\n",
    "    _, caption = data_slice\n",
    "    return tokenizer(caption)\n",
    "\n",
    "MIN_FREQ = 15\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "def build_vocab(vocab=None):\n",
    "    if not vocab:\n",
    "        vocab = build_vocab_from_iterator(\n",
    "            iterator=map(tokenize, train_ds),\n",
    "            min_freq=MIN_FREQ,\n",
    "            specials=special_symbols,\n",
    "        )\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab()\n",
    "save_vocab(vocab)\n",
    "print('Number of words in vocabulary:', len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c491633",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "itos = vocab.get_itos()\n",
    "\n",
    "def collate_fn(batch):\n",
    "    image_batch, target_batch = [], []\n",
    "    for image, caption in batch:\n",
    "        text_token_ids = text_pipeline(caption)\n",
    "        text_tensor = torch.cat((torch.tensor([BOS_IDX]),\n",
    "                                 torch.tensor(text_token_ids),\n",
    "                                 torch.tensor([EOS_IDX])))\n",
    "        target_batch.append(text_tensor)\n",
    "        image_batch.append(image.unsqueeze(0))\n",
    "        #print([itos[token] for token in text_tensor])\n",
    "        \n",
    "    image_batch = torch.cat(image_batch)\n",
    "    target_batch = pad_sequence(target_batch, batch_first=True, padding_value=PAD_IDX)\n",
    "    #for target in target_batch:\n",
    "        #print([itos[token] for token in target])\n",
    "    \n",
    "    return image_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dca0cd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor images, captions in train_loader:\\n    for caption in captions:\\n        for token in caption:\\n            print(itos[token])\\n    print(image.size())\\n    #print(caption)\\n    break\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "'''\n",
    "for images, captions in train_loader:\n",
    "    for caption in captions:\n",
    "        for token in caption:\n",
    "            print(itos[token])\n",
    "    print(image.size())\n",
    "    #print(caption)\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa702e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_size:int=1024):\n",
    "        super(Encoder, self).__init__()\n",
    "    \n",
    "        # Pretrained image classifier ResNet-152\n",
    "        self.CNN = models.resnet152(pretrained=True, progress=False)\n",
    "        for param in self.CNN.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        self.CNN.classifier = nn.Linear(self.CNN.fc.in_features, self.CNN.fc.in_features)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        self.fc2 = nn.Linear(self.CNN.fc.out_features, embed_size)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        x = F.relu(self.CNN(images))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65c33820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.embed_size, hidden_size=self.hidden_size,\n",
    "                           num_layers=self.num_layers, batch_first=True)\n",
    "        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)\n",
    "        self.linear = nn.Linear(hidden_size, self.vocab_size)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "            embeddings = self.embed(captions)\n",
    "            #print('caption size:', captions.size())\n",
    "            #embeddings = torch.permute(embeddings, (1, 0, 2))\n",
    "            features = features.unsqueeze(1)\n",
    "            #print('features size:', features.size())\n",
    "            #print('embedding size:', embeddings.size())\n",
    "            embeddings = torch.cat((features, embeddings), dim=1)\n",
    "            hidden_states, cell_states = self.lstm(embeddings)\n",
    "            outputs = self.linear(hidden_states)\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36a66177",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(CaptionModel, self).__init__()\n",
    "        self.encoder = Encoder(embed_size)\n",
    "        self.decoder = Decoder(embed_size, hidden_size, vocab_size, num_layers)\n",
    "        \n",
    "    def forward(self, image, caption):\n",
    "        x = self.encoder(image)\n",
    "        x = self.decoder(x, caption)\n",
    "        return x\n",
    "    \n",
    "    def generate_caption(self, image, vocabulary, max_length=50):\n",
    "        generated_caption = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = self.encoder(image).unsqueeze(0)\n",
    "            cell_states = None\n",
    "            \n",
    "            for i in range (max_length):\n",
    "                hidden_states, cell_states = self.decoder.lstm(x, cell_states)\n",
    "                output = self.decoder.linear(hidden_states.squeeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "                print(predicted.shape)\n",
    "                print(predicted)\n",
    "                generated_caption.append(predicted.item())\n",
    "                x = self.decoder.embed(output.long()).unsqueeze(0)\n",
    "                \n",
    "                if itos[predicted.item()] == '<eos>':\n",
    "                    break\n",
    "        return [itos[i] for i in generated_caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dc2fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 512\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "\n",
    "model = CaptionModel(\n",
    "    embed_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    vocab_size=len(vocab),\n",
    "    num_layers=5)\n",
    "\n",
    "#model.load_state_dict(torch.load(os.path.join(CWD, 'caption_model.pt'))) \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28f93999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "    \n",
    "    for i, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        #print('inputs shape:', inputs.size())\n",
    "        #print('labels shape:', labels.size())\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels[:,:-1])\n",
    "        #print('outputs size:', outputs.size())\n",
    "        loss = criterion(outputs.reshape(-1, outputs.shape[2]), labels.reshape(-1))\n",
    "        #loss = criterion(score.reshape(-1, score.shape[2]), labels.reshape(-1))\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "            \n",
    "    return last_loss\n",
    "\n",
    "def val_one_epoch(train_loss):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs, labels[:,:-1])\n",
    "            loss = criterion(outputs.reshape(-1, outputs.shape[2]), labels.reshape(-1))\n",
    "            \n",
    "            running_loss += loss.item() \n",
    "            \n",
    "    val_loss = running_loss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(train_loss, val_loss))\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa4880dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\water\\AppData\\Local\\Temp\\ipykernel_13632\\145555903.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a89acace7064634bace9abe2cdfd6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1000 loss: 1.4055679858326913\n",
      "  batch 2000 loss: 1.421873130083084\n",
      "  batch 3000 loss: 1.417212249815464\n",
      "  batch 4000 loss: 1.4182838183045388\n",
      "  batch 5000 loss: 1.4173676217198372\n",
      "  batch 6000 loss: 1.4194575473070146\n",
      "  batch 7000 loss: 1.3948413966894149\n",
      "LOSS train 1.3948413966894149 valid 1.4642400065550027\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "min_loss = 1000\n",
    "for epoch in range(EPOCHS):\n",
    "    print('Epoch number:', epoch)\n",
    "    train_loss = train_one_epoch()\n",
    "    val_loss = val_one_epoch(train_loss)\n",
    "    \n",
    "    if val_loss < min_loss:\n",
    "        save_model(model)\n",
    "print('Training Complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = 1\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(val_loader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(inputs, labels[:,:-1])\n",
    "        print(outputs)\n",
    "        if i == num_test:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d468fb1a4afb28ced3325811818b75ea47b4773e231d19fb9e9ee0fa4eb4bbba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
